defaults:
  - base
  - model@model.model: dual_ar_2_codebook_7b
  - _self_
 
project: text2semantic_pretrain_dev_audio
max_length: 2048
ckpt_path: checkpoints/consolidated.00.pth
resume_weights_only: true

trainer:
  accumulate_grad_batches: 4
  strategy:
    _target_: lightning.pytorch.strategies.DeepSpeedStrategy
    stage: 2
  gradient_clip_val: 1.0
  gradient_clip_algorithm: 'norm'
  max_steps: 100_000
  precision: bf16-true
  limit_val_batches: 10

# Dataset Configuration
tokenizer:
  _target_: transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: fishaudio/speech-lm-v1  #fishaudio/fish-speech-1

# Dataset Configuration
train_dataset:
  _target_: fish_speech.datasets.text.AutoAugTextDataset
  proto_files:
    - data/protos/train
  tokenizer: ${tokenizer}
  max_length: ${max_length}
  num_codebooks: ${model.model.config.num_codebooks}
  use_speaker: false
  interactive_prob: 0.

val_dataset:
  _target_: fish_speech.datasets.text.AutoAugTextDataset
  proto_files:
    - data/protos/test
  tokenizer: ${tokenizer}
  max_length: ${max_length}
  num_codebooks: ${model.model.config.num_codebooks}
  use_speaker: false
  interactive_prob: 0.

data:
  _target_: fish_speech.datasets.text.TextDataModule
  train_dataset: ${train_dataset}
  val_dataset: ${val_dataset}
  num_workers: 8
  batch_size: 4
  tokenizer: ${tokenizer}
  max_length: ${max_length}
  
model:
  _target_: fish_speech.models.text2semantic.TextToSemantic

  optimizer:
    _target_: torch.optim.AdamW
    _partial_: true
    lr: 1e-4
    weight_decay: 0.1
    betas: [0.9, 0.95]
    eps: 1e-5

  #save_lora_only: false
  #lora_config:
  #  _target_: fish_speech.models.text2semantic.lit_module.LoraConfig
  #  r: 16
  #  lora_alpha: 32
  
  lr_scheduler:
    _target_: torch.optim.lr_scheduler.LambdaLR
    _partial_: true
    lr_lambda:
      _target_: fish_speech.scheduler.get_cosine_schedule_with_warmup_lr_lambda
      _partial_: true
      num_warmup_steps: 1000
      num_training_steps: ${trainer.max_steps}
      final_lr_ratio: 0.1
