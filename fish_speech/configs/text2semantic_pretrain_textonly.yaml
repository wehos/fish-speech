defaults:
  - text2semantic_pretrain_small
  - _self_

project: text2semantic_pretrain_multitask_vicuna7b_nophones
max_length: 2048
ckpt_path: checkpoints/consolidated.00.pth
resume_weights_only: true

trainer:
  accumulate_grad_batches: 1
  strategy:
    _target_: lightning.pytorch.strategies.DeepSpeedStrategy
    stage: 2 
    zero_optimization: true
    pin_memory: true

train_dataset:
  _target_: fish_speech.datasets.text.AutoAugTextDataset
  use_speaker: false
  phones_prob: 0.
  interactive_prob: 0.
  use_delay_pattern: false

val_dataset:
  _target_: fish_speech.datasets.text.AutoAugTextDataset
  use_speaker: false
  phones_prob: 0.
  interactive_prob: 0.
  use_delay_pattern: false

data:
  _target_: fish_speech.datasets.text.TextDataModule
  train_dataset: ${train_dataset}
  val_dataset: ${val_dataset}
  num_workers: 6
  batch_size: 16
  tokenizer: ${tokenizer}
  max_length: ${max_length}
  
# Model Configuration
model:
  model:
    config:
      max_seq_len: 4048
      n_layer: 32
      n_head: 32
      dim: 4096

  lr_scheduler:
    _target_: torch.optim.lr_scheduler.LambdaLR
    _partial_: true
    lr_lambda:
      _target_: fish_speech.scheduler.get_cosine_schedule_with_warmup_lr_lambda
      _partial_: true
      num_warmup_steps: 2000
      num_training_steps: ${trainer.max_steps}
      final_lr_ratio: 0.1

callbacks:
  model_checkpoint:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    filename: "step_{step:09d}"
    save_last: true # additionally always save an exact copy of the last checkpoint to a file last.ckpt
    save_top_k: 0 # save 5 latest checkpoints
    every_n_train_steps: 5000 # save checkpoints every 5000 steps
