{"config":{"lang":["zh","en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\u4ecb\u7ecd","text":"<p>Warning</p> <p>\u6211\u4eec\u4e0d\u5bf9\u4ee3\u7801\u5e93\u7684\u4efb\u4f55\u975e\u6cd5\u4f7f\u7528\u627f\u62c5\u4efb\u4f55\u8d23\u4efb. \u8bf7\u53c2\u9605\u60a8\u5f53\u5730\u5173\u4e8e DMCA (\u6570\u5b57\u5343\u5e74\u6cd5\u6848) \u548c\u5176\u4ed6\u76f8\u5173\u6cd5\u5f8b\u6cd5\u89c4.</p> <p>\u6b64\u4ee3\u7801\u5e93\u6839\u636e <code>BSD-3-Clause</code> \u8bb8\u53ef\u8bc1\u53d1\u5e03, \u6240\u6709\u6a21\u578b\u6839\u636e CC-BY-NC-SA-4.0 \u8bb8\u53ef\u8bc1\u53d1\u5e03.</p> <p> </p>"},{"location":"#_2","title":"\u8981\u6c42","text":"<ul> <li>GPU\u5185\u5b58: 2GB (\u7528\u4e8e\u63a8\u7406), 16GB (\u7528\u4e8e\u5fae\u8c03)</li> <li>\u7cfb\u7edf: Linux (\u5168\u90e8\u529f\u80fd), Windows (\u4ec5\u63a8\u7406, \u4e0d\u652f\u6301 <code>flash-attn</code>, \u4e0d\u652f\u6301 <code>torch.compile</code>)</li> </ul> <p>\u56e0\u6b64, \u6211\u4eec\u5f3a\u70c8\u5efa\u8bae Windows \u7528\u6237\u4f7f\u7528 WSL2 \u6216 docker \u6765\u8fd0\u884c\u4ee3\u7801\u5e93.</p>"},{"location":"#_3","title":"\u8bbe\u7f6e","text":"<pre><code># \u521b\u5efa\u4e00\u4e2a python 3.10 \u865a\u62df\u73af\u5883, \u4f60\u4e5f\u53ef\u4ee5\u7528 virtualenv\nconda create -n fish-speech python=3.10\nconda activate fish-speech\n\n# \u5b89\u88c5 pytorch nightly \u7248\u672c\npip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121\n\n# \u5b89\u88c5 flash-attn (\u9002\u7528\u4e8elinux)\npip3 install ninja &amp;&amp; MAX_JOBS=4 pip3 install flash-attn --no-build-isolation\n\n# \u5b89\u88c5 fish-speech\npip3 install -e .\n</code></pre>"},{"location":"#_4","title":"\u66f4\u65b0\u65e5\u5fd7","text":"<ul> <li>2023/12/28: \u6dfb\u52a0\u4e86 <code>lora</code> \u5fae\u8c03\u652f\u6301.</li> <li>2023/12/27: \u6dfb\u52a0\u4e86 <code>gradient checkpointing</code>, <code>causual sampling</code> \u548c <code>flash-attn</code> \u652f\u6301.</li> <li>2023/12/19: \u66f4\u65b0\u4e86 Webui \u548c HTTP API.</li> <li>2023/12/18: \u66f4\u65b0\u4e86\u5fae\u8c03\u6587\u6863\u548c\u76f8\u5173\u4f8b\u5b50.</li> <li>2023/12/17: \u66f4\u65b0\u4e86 <code>text2semantic</code> \u6a21\u578b, \u652f\u6301\u65e0\u97f3\u7d20\u6a21\u5f0f.</li> <li>2023/12/13: \u6d4b\u8bd5\u7248\u53d1\u5e03, \u5305\u542b VQGAN \u6a21\u578b\u548c\u4e00\u4e2a\u57fa\u4e8e LLAMA \u7684\u8bed\u8a00\u6a21\u578b (\u53ea\u652f\u6301\u97f3\u7d20).</li> </ul>"},{"location":"#_5","title":"\u81f4\u8c22","text":"<ul> <li>VITS2 (daniilrobnikov)</li> <li>Bert-VITS2</li> <li>GPT VITS</li> <li>MQTTS</li> <li>GPT Fast</li> <li>Transformers</li> </ul>"},{"location":"finetune/","title":"\u5fae\u8c03","text":"<p>\u663e\u7136, \u5f53\u4f60\u6253\u5f00\u8fd9\u4e2a\u9875\u9762\u7684\u65f6\u5019, \u4f60\u5df2\u7ecf\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b few-shot \u7684\u6548\u679c\u4e0d\u7b97\u6ee1\u610f. \u4f60\u60f3\u8981\u5fae\u8c03\u4e00\u4e2a\u6a21\u578b, \u4f7f\u5f97\u5b83\u5728\u4f60\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u597d.  </p> <p><code>Fish Speech</code> \u7531\u4e24\u4e2a\u6a21\u5757\u7ec4\u6210: <code>VQGAN</code> \u548c <code>LLAMA</code>. </p> <p>Info</p> <p>\u4f60\u5e94\u8be5\u5148\u8fdb\u884c\u5982\u4e0b\u6d4b\u8bd5\u6765\u5224\u65ad\u4f60\u662f\u5426\u9700\u8981\u5fae\u8c03 <code>VQGAN</code>: <pre><code>python tools/vqgan/inference.py -i test.wav\n</code></pre> \u8be5\u6d4b\u8bd5\u4f1a\u751f\u6210\u4e00\u4e2a <code>fake.wav</code> \u6587\u4ef6, \u5982\u679c\u8be5\u6587\u4ef6\u7684\u97f3\u8272\u548c\u8bf4\u8bdd\u4eba\u7684\u97f3\u8272\u4e0d\u540c, \u6216\u8005\u8d28\u91cf\u4e0d\u9ad8, \u4f60\u9700\u8981\u5fae\u8c03 <code>VQGAN</code>.</p> <p>\u76f8\u5e94\u7684, \u4f60\u53ef\u4ee5\u53c2\u8003 \u63a8\u7406 \u6765\u8fd0\u884c <code>generate.py</code>, \u5224\u65ad\u97f5\u5f8b\u662f\u5426\u6ee1\u610f, \u5982\u679c\u4e0d\u6ee1\u610f, \u5219\u9700\u8981\u5fae\u8c03 <code>LLAMA</code>.</p>"},{"location":"finetune/#vqgan","title":"VQGAN \u5fae\u8c03","text":""},{"location":"finetune/#1","title":"1. \u51c6\u5907\u6570\u636e\u96c6","text":"<pre><code>.\n\u251c\u2500\u2500 SPK1\n\u2502   \u251c\u2500\u2500 21.15-26.44.mp3\n\u2502   \u251c\u2500\u2500 27.51-29.98.mp3\n\u2502   \u2514\u2500\u2500 30.1-32.71.mp3\n\u2514\u2500\u2500 SPK2\n    \u2514\u2500\u2500 38.79-40.85.mp3\n</code></pre> <p>\u4f60\u9700\u8981\u5c06\u6570\u636e\u96c6\u8f6c\u4e3a\u4ee5\u4e0a\u683c\u5f0f, \u5e76\u653e\u5230 <code>data/demo</code> \u4e0b, \u97f3\u9891\u540e\u7f00\u53ef\u4ee5\u4e3a <code>.mp3</code>, <code>.wav</code> \u6216 <code>.flac</code>.</p>"},{"location":"finetune/#2","title":"2. \u5206\u5272\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6","text":"<pre><code>python tools/vqgan/create_train_split.py data/demo\n</code></pre> <p>\u8be5\u547d\u4ee4\u4f1a\u5728 <code>data/demo</code> \u76ee\u5f55\u4e0b\u521b\u5efa <code>data/demo/vq_train_filelist.txt</code> \u548c <code>data/demo/vq_val_filelist.txt</code> \u6587\u4ef6, \u5206\u522b\u7528\u4e8e\u8bad\u7ec3\u548c\u9a8c\u8bc1.  </p> <p>Info</p> <p>\u5bf9\u4e8e VITS \u683c\u5f0f, \u4f60\u53ef\u4ee5\u4f7f\u7528 <code>--filelist xxx.list</code> \u6765\u6307\u5b9a\u6587\u4ef6\u5217\u8868. \u8bf7\u6ce8\u610f, <code>filelist</code> \u6240\u6307\u5411\u7684\u97f3\u9891\u6587\u4ef6\u5fc5\u987b\u4e5f\u4f4d\u4e8e <code>data/demo</code> \u6587\u4ef6\u5939\u4e0b.</p>"},{"location":"finetune/#3","title":"3. \u542f\u52a8\u8bad\u7ec3","text":"<pre><code>python fish_speech/train.py --config-name vqgan_finetune\n</code></pre> <p>Note</p> <p>\u4f60\u53ef\u4ee5\u901a\u8fc7\u4fee\u6539 <code>fish_speech/configs/vqgan_finetune.yaml</code> \u6765\u4fee\u6539\u8bad\u7ec3\u53c2\u6570, \u4f46\u5927\u90e8\u5206\u60c5\u51b5\u4e0b, \u4f60\u4e0d\u9700\u8981\u8fd9\u4e48\u505a.</p>"},{"location":"finetune/#4","title":"4. \u6d4b\u8bd5\u97f3\u9891","text":"<pre><code>python tools/vqgan/inference.py -i test.wav --checkpoint-path results/vqgan_finetune/checkpoints/step_000010000.ckpt\n</code></pre> <p>\u4f60\u53ef\u4ee5\u67e5\u770b <code>fake.wav</code> \u6765\u5224\u65ad\u5fae\u8c03\u6548\u679c.</p> <p>Note</p> <p>\u4f60\u4e5f\u53ef\u4ee5\u5c1d\u8bd5\u5176\u4ed6\u7684 checkpoint, \u6211\u4eec\u5efa\u8bae\u4f60\u4f7f\u7528\u6700\u65e9\u7684\u6ee1\u8db3\u4f60\u8981\u6c42\u7684 checkpoint, \u4ed6\u4eec\u901a\u5e38\u5728 OOD \u4e0a\u8868\u73b0\u66f4\u597d.</p>"},{"location":"finetune/#llama","title":"LLAMA \u5fae\u8c03","text":""},{"location":"finetune/#1_1","title":"1. \u51c6\u5907\u6570\u636e\u96c6","text":"<pre><code>.\n\u251c\u2500\u2500 SPK1\n\u2502   \u251c\u2500\u2500 21.15-26.44.lab\n\u2502   \u251c\u2500\u2500 21.15-26.44.mp3\n\u2502   \u251c\u2500\u2500 27.51-29.98.lab\n\u2502   \u251c\u2500\u2500 27.51-29.98.mp3\n\u2502   \u251c\u2500\u2500 30.1-32.71.lab\n\u2502   \u2514\u2500\u2500 30.1-32.71.mp3\n\u2514\u2500\u2500 SPK2\n    \u251c\u2500\u2500 38.79-40.85.lab\n    \u2514\u2500\u2500 38.79-40.85.mp3\n</code></pre> <p>\u4f60\u9700\u8981\u5c06\u6570\u636e\u96c6\u8f6c\u4e3a\u4ee5\u4e0a\u683c\u5f0f, \u5e76\u653e\u5230 <code>data/demo</code> \u4e0b, \u97f3\u9891\u540e\u7f00\u53ef\u4ee5\u4e3a <code>.mp3</code>, <code>.wav</code> \u6216 <code>.flac</code>, \u6807\u6ce8\u6587\u4ef6\u540e\u7f00\u53ef\u4ee5\u4e3a <code>.lab</code> \u6216 <code>.txt</code>.</p> <p>Note</p> <p>\u4f60\u53ef\u4ee5\u901a\u8fc7\u4fee\u6539 <code>fish_speech/configs/data/finetune.yaml</code> \u6765\u4fee\u6539\u6570\u636e\u96c6\u8def\u5f84, \u4ee5\u53ca\u6df7\u5408\u6570\u636e\u96c6.</p> <p>Warning</p> <p>\u5efa\u8bae\u5148\u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u54cd\u5ea6\u5339\u914d, \u4f60\u53ef\u4ee5\u4f7f\u7528 fish-audio-preprocess \u6765\u5b8c\u6210\u8fd9\u4e00\u6b65\u9aa4.  <pre><code>fap loudness-norm demo-raw demo --clean\n</code></pre></p>"},{"location":"finetune/#2-token","title":"2. \u6279\u91cf\u63d0\u53d6\u8bed\u4e49 token","text":"<p>\u786e\u4fdd\u4f60\u5df2\u7ecf\u4e0b\u8f7d\u4e86 vqgan \u6743\u91cd, \u5982\u679c\u6ca1\u6709, \u8bf7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4:</p> <pre><code>huggingface-cli download fishaudio/speech-lm-v1 vqgan-v1.pth --local-dir checkpoints\n</code></pre> <p>\u5bf9\u4e8e\u4e2d\u56fd\u5927\u9646\u7528\u6237, \u53ef\u4f7f\u7528 mirror \u4e0b\u8f7d.</p> <pre><code>HF_ENDPOINT=https://hf-mirror.com huggingface-cli download fishaudio/speech-lm-v1 vqgan-v1.pth --local-dir checkpoints\n</code></pre> <p>\u968f\u540e\u53ef\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u63d0\u53d6\u8bed\u4e49 token:</p> <pre><code>python tools/vqgan/extract_vq.py data/demo \\\n    --num-workers 1 --batch-size 16 \\\n    --config-name \"vqgan_pretrain\" \\\n    --checkpoint-path \"checkpoints/vqgan-v1.pth\"\n</code></pre> <p>Note</p> <p>\u4f60\u53ef\u4ee5\u8c03\u6574 <code>--num-workers</code> \u548c <code>--batch-size</code> \u6765\u63d0\u9ad8\u63d0\u53d6\u901f\u5ea6, \u4f46\u662f\u8bf7\u6ce8\u610f\u4e0d\u8981\u8d85\u8fc7\u4f60\u7684\u663e\u5b58\u9650\u5236. \u5bf9\u4e8e VITS \u683c\u5f0f, \u4f60\u53ef\u4ee5\u4f7f\u7528 <code>--filelist xxx.list</code> \u6765\u6307\u5b9a\u6587\u4ef6\u5217\u8868.</p> <p>\u8be5\u547d\u4ee4\u4f1a\u5728 <code>data/demo</code> \u76ee\u5f55\u4e0b\u521b\u5efa <code>.npy</code> \u6587\u4ef6, \u5982\u4e0b\u6240\u793a:</p> <pre><code>.\n\u251c\u2500\u2500 SPK1\n\u2502   \u251c\u2500\u2500 21.15-26.44.lab\n\u2502   \u251c\u2500\u2500 21.15-26.44.mp3\n\u2502   \u251c\u2500\u2500 21.15-26.44.npy\n\u2502   \u251c\u2500\u2500 27.51-29.98.lab\n\u2502   \u251c\u2500\u2500 27.51-29.98.mp3\n\u2502   \u251c\u2500\u2500 27.51-29.98.npy\n\u2502   \u251c\u2500\u2500 30.1-32.71.lab\n\u2502   \u251c\u2500\u2500 30.1-32.71.mp3\n\u2502   \u2514\u2500\u2500 30.1-32.71.npy\n\u2514\u2500\u2500 SPK2\n    \u251c\u2500\u2500 38.79-40.85.lab\n    \u251c\u2500\u2500 38.79-40.85.mp3\n    \u2514\u2500\u2500 38.79-40.85.npy\n</code></pre>"},{"location":"finetune/#3-protobuf","title":"3. \u6253\u5305\u6570\u636e\u96c6\u4e3a protobuf","text":"<pre><code>python tools/llama/build_dataset.py \\\n    --config \"fish_speech/configs/data/finetune.yaml\" \\\n    --output \"data/quantized-dataset-ft.protos\" \\\n    --num-workers 16\n</code></pre> <p>\u547d\u4ee4\u6267\u884c\u5b8c\u6bd5\u540e, \u4f60\u5e94\u8be5\u80fd\u5728 <code>data</code> \u76ee\u5f55\u4e0b\u770b\u5230 <code>quantized-dataset-ft.protos</code> \u6587\u4ef6.</p> <p>Note</p> <p>\u5bf9\u4e8e VITS \u683c\u5f0f, \u4f60\u53ef\u4ee5\u4f7f\u7528 <code>--filelist xxx.list</code> \u6765\u6307\u5b9a\u6587\u4ef6\u5217\u8868.</p>"},{"location":"finetune/#4-rust","title":"4. \u542f\u52a8 Rust \u6570\u636e\u670d\u52a1\u5668","text":"<p>\u7531\u4e8e\u52a0\u8f7d\u548c\u6253\u4e71\u6570\u636e\u96c6\u975e\u5e38\u7f13\u6162\u4e14\u5360\u7528\u5185\u5b58, \u56e0\u6b64\u6211\u4eec\u4f7f\u7528 rust \u670d\u52a1\u5668\u6765\u52a0\u8f7d\u548c\u6253\u4e71\u6570\u636e. \u8be5\u670d\u52a1\u5668\u57fa\u4e8e GRPC, \u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u5b89\u88c5:</p> <pre><code>cd data_server\ncargo build --release\n</code></pre> <p>\u7f16\u8bd1\u5b8c\u6210\u540e\u4f60\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u6765\u542f\u52a8\u670d\u52a1\u5668:</p> <pre><code>export RUST_LOG=info # \u53ef\u9009, \u7528\u4e8e\u8c03\u8bd5\ndata_server/target/release/data_server \\\n    --files \"data/quantized-dataset-ft.protos\" \n</code></pre> <p>Note</p> <p>\u4f60\u53ef\u4ee5\u6307\u5b9a\u591a\u4e2a <code>--files</code> \u53c2\u6570\u6765\u52a0\u8f7d\u591a\u4e2a\u6570\u636e\u96c6.</p>"},{"location":"finetune/#5","title":"5. \u6700\u540e, \u542f\u52a8\u5fae\u8c03","text":"<p>\u540c\u6837\u7684, \u8bf7\u786e\u4fdd\u4f60\u5df2\u7ecf\u4e0b\u8f7d\u4e86 <code>LLAMA</code> \u6743\u91cd, \u5982\u679c\u6ca1\u6709, \u8bf7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4:</p> <pre><code>huggingface-cli download fishaudio/speech-lm-v1 text2semantic-400m-v0.2-4k.pth --local-dir checkpoints\n</code></pre> <p>\u5bf9\u4e8e\u4e2d\u56fd\u5927\u9646\u7528\u6237, \u53ef\u4f7f\u7528 mirror \u4e0b\u8f7d.</p> <pre><code>HF_ENDPOINT=https://hf-mirror.com huggingface-cli download fishaudio/speech-lm-v1 text2semantic-400m-v0.2-4k.pth --local-dir checkpoints\n</code></pre> <p>\u6700\u540e, \u4f60\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u542f\u52a8\u5fae\u8c03:</p> <pre><code>python fish_speech/train.py --config-name text2semantic_finetune\n</code></pre> <p>Note</p> <p>\u5982\u679c\u4f60\u60f3\u4f7f\u7528 lora, \u8bf7\u4f7f\u7528 <code>--config-name text2semantic_finetune_lora</code> \u6765\u542f\u52a8\u5fae\u8c03.</p> <p>Note</p> <p>\u4f60\u53ef\u4ee5\u901a\u8fc7\u4fee\u6539 <code>fish_speech/configs/text2semantic_finetune.yaml</code> \u6765\u4fee\u6539\u8bad\u7ec3\u53c2\u6570\u5982 <code>batch_size</code>, <code>gradient_accumulation_steps</code> \u7b49, \u6765\u9002\u5e94\u4f60\u7684\u663e\u5b58.</p> <p>\u8bad\u7ec3\u7ed3\u675f\u540e, \u4f60\u53ef\u4ee5\u53c2\u8003 \u63a8\u7406 \u90e8\u5206, \u5e76\u643a\u5e26 <code>--speaker SPK1</code> \u53c2\u6570\u6765\u6d4b\u8bd5\u4f60\u7684\u6a21\u578b.</p> <p>Info</p> <p>\u9ed8\u8ba4\u914d\u7f6e\u4e0b, \u57fa\u672c\u53ea\u4f1a\u5b66\u5230\u8bf4\u8bdd\u4eba\u7684\u53d1\u97f3\u65b9\u5f0f, \u800c\u4e0d\u5305\u542b\u97f3\u8272, \u4f60\u4f9d\u7136\u9700\u8981\u4f7f\u7528 prompt \u6765\u4fdd\u8bc1\u97f3\u8272\u7684\u7a33\u5b9a\u6027. \u5982\u679c\u4f60\u60f3\u8981\u5b66\u5230\u97f3\u8272, \u8bf7\u5c06\u8bad\u7ec3\u6b65\u6570\u8c03\u5927, \u4f46\u8fd9\u6709\u53ef\u80fd\u4f1a\u5bfc\u81f4\u8fc7\u62df\u5408.</p>"},{"location":"inference/","title":"\u63a8\u7406","text":"<p>\u63a8\u7406\u652f\u6301\u547d\u4ee4\u884c, http api, \u4ee5\u53ca webui \u4e09\u79cd\u65b9\u5f0f.  </p> <p>Note</p> <p>\u603b\u7684\u6765\u8bf4, \u63a8\u7406\u5206\u4e3a\u51e0\u4e2a\u90e8\u5206:  </p> <ol> <li>\u7ed9\u5b9a\u4e00\u6bb5 5-10 \u79d2\u7684\u8bed\u97f3, \u5c06\u5b83\u7528 VQGAN \u7f16\u7801.  </li> <li>\u5c06\u7f16\u7801\u540e\u7684\u8bed\u4e49 token \u548c\u5bf9\u5e94\u6587\u672c\u8f93\u5165\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u4f8b\u5b50.  </li> <li>\u7ed9\u5b9a\u4e00\u6bb5\u65b0\u6587\u672c, \u8ba9\u6a21\u578b\u751f\u6210\u5bf9\u5e94\u7684\u8bed\u4e49 token.  </li> <li>\u5c06\u751f\u6210\u7684\u8bed\u4e49 token \u8f93\u5165 VQGAN \u89e3\u7801, \u751f\u6210\u5bf9\u5e94\u7684\u8bed\u97f3.  </li> </ol>"},{"location":"inference/#_2","title":"\u547d\u4ee4\u884c\u63a8\u7406","text":"<p>\u4ece\u6211\u4eec\u7684 huggingface \u4ed3\u5e93\u4e0b\u8f7d\u6240\u9700\u7684 <code>vqgan</code> \u548c <code>text2semantic</code> \u6a21\u578b\u3002</p> <p><pre><code>huggingface-cli download fishaudio/speech-lm-v1 vqgan-v1.pth --local-dir checkpoints\nhuggingface-cli download fishaudio/speech-lm-v1 text2semantic-400m-v0.2-4k.pth --local-dir checkpoints\n</code></pre> \u5bf9\u4e8e\u4e2d\u56fd\u5927\u9646\u7528\u6237\uff0c\u53ef\u4f7f\u7528mirror\u4e0b\u8f7d\u3002 <pre><code>HF_ENDPOINT=https://hf-mirror.com huggingface-cli download fishaudio/speech-lm-v1 vqgan-v1.pth --local-dir checkpoints\nHF_ENDPOINT=https://hf-mirror.com huggingface-cli download fishaudio/speech-lm-v1 text2semantic-400m-v0.2-4k.pth --local-dir checkpoints\n</code></pre></p>"},{"location":"inference/#1-prompt","title":"1. \u4ece\u8bed\u97f3\u751f\u6210 prompt:","text":"<p>Note</p> <p>\u5982\u679c\u4f60\u6253\u7b97\u8ba9\u6a21\u578b\u968f\u673a\u9009\u62e9\u97f3\u8272, \u4f60\u53ef\u4ee5\u8df3\u8fc7\u8fd9\u4e00\u6b65.</p> <p><pre><code>python tools/vqgan/inference.py \\\n    -i \"paimon.wav\" \\\n    --checkpoint-path \"checkpoints/vqgan-v1.pth\"\n</code></pre> \u4f60\u5e94\u8be5\u80fd\u5f97\u5230\u4e00\u4e2a <code>fake.npy</code> \u6587\u4ef6.</p>"},{"location":"inference/#2-token","title":"2. \u4ece\u6587\u672c\u751f\u6210\u8bed\u4e49 token:","text":"<pre><code>python tools/llama/generate.py \\\n    --text \"\u8981\u8f6c\u6362\u7684\u6587\u672c\" \\\n    --prompt-text \"\u4f60\u7684\u53c2\u8003\u6587\u672c\" \\\n    --prompt-tokens \"fake.npy\" \\\n    --checkpoint-path \"checkpoints/text2semantic-400m-v0.2-4k.pth\" \\\n    --num-samples 2 \\\n    --compile\n</code></pre> <p>\u8be5\u547d\u4ee4\u4f1a\u5728\u5de5\u4f5c\u76ee\u5f55\u4e0b\u521b\u5efa <code>codes_N</code> \u6587\u4ef6, \u5176\u4e2d N \u662f\u4ece 0 \u5f00\u59cb\u7684\u6574\u6570.</p> <p>Note</p> <p>\u60a8\u53ef\u80fd\u5e0c\u671b\u4f7f\u7528 <code>--compile</code> \u6765\u878d\u5408 cuda \u5185\u6838\u4ee5\u5b9e\u73b0\u66f4\u5feb\u7684\u63a8\u7406 (~30 \u4e2a token/\u79d2 -&gt; ~500 \u4e2a token/\u79d2). \u5bf9\u5e94\u7684, \u5982\u679c\u4f60\u4e0d\u6253\u7b97\u4f7f\u7528\u52a0\u901f, \u4f60\u53ef\u4ee5\u6ce8\u91ca\u6389 <code>--compile</code> \u53c2\u6570.</p> <p>Info</p> <p>\u5bf9\u4e8e\u4e0d\u652f\u6301 bf16 \u7684 GPU, \u4f60\u53ef\u80fd\u9700\u8981\u4f7f\u7528 <code>--half</code> \u53c2\u6570.</p> <p>Warning</p> <p>\u5982\u679c\u4f60\u5728\u4f7f\u7528\u81ea\u5df1\u5fae\u8c03\u7684\u6a21\u578b, \u8bf7\u52a1\u5fc5\u643a\u5e26 <code>--speaker</code> \u53c2\u6570\u6765\u4fdd\u8bc1\u53d1\u97f3\u7684\u7a33\u5b9a\u6027. \u5982\u679c\u4f60\u4f7f\u7528\u4e86 lora, \u8bf7\u4f7f\u7528 <code>--config-name text2semantic_finetune_lora</code> \u6765\u52a0\u8f7d\u6a21\u578b.</p>"},{"location":"inference/#3-token","title":"3. \u4ece\u8bed\u4e49 token \u751f\u6210\u4eba\u58f0:","text":"<pre><code>python tools/vqgan/inference.py \\\n    -i \"codes_0.npy\" \\\n    --checkpoint-path \"checkpoints/vqgan-v1.pth\"\n</code></pre>"},{"location":"inference/#http-api","title":"HTTP API \u63a8\u7406","text":"<p>\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u542f\u52a8 HTTP \u670d\u52a1:</p> <pre><code>python -m zibai tools.api_server:app --listen 127.0.0.1:8000\n# \u63a8\u8350\u4e2d\u56fd\u5927\u9646\u7528\u6237\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u542f\u52a8 HTTP \u670d\u52a1:\nHF_ENDPOINT=https://hf-mirror.com python -m zibai tools.api_server:app --listen 127.0.0.1:8000\n</code></pre> <p>\u968f\u540e, \u4f60\u53ef\u4ee5\u5728 <code>http://127.0.0.1:8000/docs</code> \u4e2d\u67e5\u770b\u5e76\u6d4b\u8bd5 API. \u4e00\u822c\u6765\u8bf4, \u4f60\u9700\u8981\u5148\u8c03\u7528 <code>PUT /v1/models/default</code> \u6765\u52a0\u8f7d\u6a21\u578b, \u7136\u540e\u8c03\u7528 <code>POST /v1/models/default/invoke</code> \u6765\u8fdb\u884c\u63a8\u7406. \u5177\u4f53\u7684\u53c2\u6570\u8bf7\u53c2\u8003 API \u6587\u6863.</p>"},{"location":"inference/#webui","title":"WebUI \u63a8\u7406","text":"<p>\u5728\u8fd0\u884c WebUI \u4e4b\u524d, \u4f60\u9700\u8981\u5148\u542f\u52a8 HTTP \u670d\u52a1, \u5982\u4e0a\u6240\u8ff0.</p> <p>\u968f\u540e\u4f60\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u6765\u542f\u52a8 WebUI:</p> <pre><code>python fish_speech/webui/app.py\n</code></pre> <p>\u6216\u9644\u5e26\u53c2\u6570\u6765\u542f\u52a8 WebUI:</p> <pre><code># \u4ee5\u4e34\u65f6\u73af\u5883\u53d8\u91cf\u7684\u65b9\u5f0f\u542f\u52a8:\nGRADIO_SERVER_NAME=127.0.0.1 GRADIO_SERVER_PORT=7860 python fish_speech/webui/app.py\n</code></pre> <p>\u795d\u5927\u5bb6\u73a9\u5f97\u5f00\u5fc3!</p>"},{"location":"samples/","title":"\u4f8b\u5b50","text":"<p>Note</p> <p>\u7531\u4e8e\u65e5\u82f1\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3, \u6211\u4eec\u5148\u5c06\u6587\u672c\u97f3\u7d20\u5316, \u518d\u7528\u4e8e\u751f\u6210.</p>"},{"location":"samples/#1","title":"\u4e2d\u6587\u53e5\u5b50 1","text":"<pre><code>\u4eba\u95f4\u706f\u706b\u5012\u6620\u6e56\u4e2d\uff0c\u5979\u7684\u6e34\u671b\u8ba9\u9759\u6c34\u6cdb\u8d77\u6d9f\u6f2a\u3002\u82e5\u4ee3\u4ef7\u53ea\u662f\u5b64\u72ec\uff0c\u90a3\u5c31\u8ba9\u8fd9\u4efd\u613f\u671b\u8086\u610f\u6d41\u6dcc\u3002\n\u6d41\u5165\u5979\u6240\u6ce8\u89c6\u7684\u4e16\u95f4\uff0c\u4e5f\u6d41\u5165\u5979\u5982\u6e56\u6c34\u822c\u6f84\u6f88\u7684\u76ee\u5149\u3002\n</code></pre> \u8bf4\u8bdd\u4eba \u8f93\u5165\u97f3\u9891 \u5408\u6210\u97f3\u9891 \u7eb3\u897f\u59b2 (\u539f\u795e) \u949f\u79bb (\u539f\u795e) \u8299\u5b81\u5a1c (\u539f\u795e) \u968f\u673a\u8bf4\u8bdd\u4eba 1  -  \u968f\u673a\u8bf4\u8bdd\u4eba 2  -"},{"location":"samples/#2","title":"\u4e2d\u6587\u53e5\u5b50 2 (\u957f\u53e5)","text":"<pre><code>\u4f60\u4eec\u8fd9\u4e2a\u662f\u4ec0\u4e48\u7fa4\u554a\uff0c\u4f60\u4eec\u8fd9\u662f\u5bb3\u4eba\u4e0d\u6d45\u554a\u4f60\u4eec\u8fd9\u4e2a\u7fa4\uff01\u8c01\u662f\u7fa4\u4e3b\uff0c\u51fa\u6765\uff01\u771f\u7684\u592a\u8fc7\u5206\u4e86\u3002\u4f60\u4eec\u641e\u8fd9\u4e2a\u7fa4\u5e72\u4ec0\u4e48\uff1f\n\u6211\u513f\u5b50\u6bcf\u4e00\u79d1\u7684\u6210\u7ee9\u90fd\u4e0d\u8fc7\u90a3\u4e2a\u5e73\u5747\u5206\u5450\uff0c\u4ed6\u73b0\u5728\u521d\u4e8c\uff0c\u4f60\u53eb\u6211\u513f\u5b50\u600e\u4e48\u529e\u554a\uff1f\u4ed6\u73b0\u5728\u8fd8\u4e0d\u5230\u9ad8\u4e2d\u554a\uff1f\n\u4f60\u4eec\u5bb3\u6b7b\u6211\u513f\u5b50\u4e86\uff01\u5feb\u70b9\u51fa\u6765\u4f60\u8fd9\u4e2a\u7fa4\u4e3b\uff01\u518d\u8fd9\u6837\u6211\u53bb\u62a5\u8b66\u4e86\u554a\uff01\u6211\u8ddf\u4f60\u4eec\u8bf4\u4f60\u4eec\u8fd9\u4e00\u5e2e\u4eba\u554a\uff0c\u4e00\u5929\u5230\u665a\u554a\uff0c\n\u641e\u8fd9\u4e9b\u4ec0\u4e48\u6e38\u620f\u554a\uff0c\u52a8\u6f2b\u554a\uff0c\u4f1a\u5bb3\u6b7b\u4f60\u4eec\u7684\uff0c\u4f60\u4eec\u6ca1\u6709\u524d\u9014\u6211\u8ddf\u4f60\u8bf4\u3002\u4f60\u4eec\u8fd9\u4e5d\u767e\u591a\u4e2a\u4eba\uff0c\u597d\u597d\u5b66\u4e60\u4e0d\u597d\u5417\uff1f\n\u4e00\u5929\u5230\u665a\u5728\u4e0a\u7f51\u3002\u6709\u4ec0\u4e48\u610f\u601d\u554a\uff1f\u9ebb\u70e6\u4f60\u91cd\u89c6\u4e00\u4e0b\u4f60\u4eec\u7684\u751f\u6d3b\u7684\u76ee\u6807\u554a\uff1f\u6709\u4e00\u70b9\u5b66\u4e60\u76ee\u6807\u884c\u4e0d\u884c\uff1f\u4e00\u5929\u5230\u665a\u4e0a\u7f51\u662f\u4e0d\u662f\u4eba\u554a\uff1f\n</code></pre> \u8bf4\u8bdd\u4eba \u8f93\u5165\u97f3\u9891 \u5408\u6210\u97f3\u9891 \u7eb3\u897f\u59b2 (\u539f\u795e) \u9752\u96c0 (\u661f\u7a79\u94c1\u9053)"},{"location":"samples/#_2","title":"\u82f1\u6587\u53e5\u5b50","text":"<pre><code>In the realm of advanced technology, the evolution of artificial intelligence stands as a \nmonumental achievement. This dynamic field, constantly pushing the boundaries of what \nmachines can do, has seen rapid growth and innovation. From deciphering complex data \npatterns to driving cars autonomously, AI's applications are vast and diverse.\n</code></pre> \u8bf4\u8bdd\u4eba \u8f93\u5165\u97f3\u9891 \u5408\u6210\u97f3\u9891 Speaker 200 (LibriTTS) \u968f\u673a\u8bf4\u8bdd\u4eba 1  -  \u968f\u673a\u8bf4\u8bdd\u4eba 2  -"},{"location":"samples/#_3","title":"\u65e5\u6587\u53e5\u5b50","text":"<pre><code>\u5148\u9032\u6280\u8853\u306e\u9818\u57df\u306b\u304a\u3044\u3066\u3001\u4eba\u5de5\u77e5\u80fd\u306e\u9032\u5316\u306f\u753b\u671f\u7684\u306a\u6210\u679c\u3068\u3057\u3066\u7acb\u3063\u3066\u3044\u307e\u3059\u3002\u5e38\u306b\u6a5f\u68b0\u304c\u3067\u304d\u308b\u3053\u3068\u306e\u9650\u754c\u3092\n\u62bc\u3057\u5e83\u3052\u3066\u3044\u308b\u3053\u306e\u30c0\u30a4\u30ca\u30df\u30c3\u30af\u306a\u5206\u91ce\u306f\u3001\u6025\u901f\u306a\u6210\u9577\u3068\u9769\u65b0\u3092\u898b\u305b\u3066\u3044\u307e\u3059\u3002\u8907\u96d1\u306a\u30c7\u30fc\u30bf\u30d1\u30bf\u30fc\u30f3\u306e\u89e3\u8aad\u304b\n\u3089\u81ea\u52d5\u904b\u8ee2\u8eca\u306e\u64cd\u7e26\u307e\u3067\u3001AI\u306e\u5fdc\u7528\u306f\u5e83\u7bc4\u56f2\u306b\u53ca\u3073\u307e\u3059\u3002\n</code></pre> \u8bf4\u8bdd\u4eba \u8f93\u5165\u97f3\u9891 \u5408\u6210\u97f3\u9891 \u968f\u673a\u8bf4\u8bdd\u4eba 1  -  \u968f\u673a\u8bf4\u8bdd\u4eba 2  -"},{"location":"en/","title":"Introduction","text":"<p>Warning</p> <p>We assume no responsibility for any illegal use of the codebase. Please refer to the local laws regarding DMCA (Digital Millennium Copyright Act) and other relevant laws in your area.</p> <p>This codebase is released under the <code>BSD-3-Clause</code> license, and all models are released under the CC-BY-NC-SA-4.0 license.</p> <p> </p>"},{"location":"en/#requirements","title":"Requirements","text":"<ul> <li>GPU Memory: 2GB (for inference), 16GB (for fine-tuning)</li> <li>System: Linux (full functionality), Windows (inference only, no support for <code>flash-attn</code>, no support for <code>torch.compile</code>)</li> </ul> <p>Therefore, we strongly recommend Windows users to use WSL2 or docker to run the codebase.</p>"},{"location":"en/#setup","title":"Setup","text":"<pre><code># Create a python 3.10 virtual environment, you can also use virtualenv\nconda create -n fish-speech python=3.10\nconda activate fish-speech\n\n# Install pytorch nightly\npip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121\n\n# Install flash-attn (for Linux)\npip3 install ninja &amp;&amp; MAX_JOBS=4 pip3 install flash-attn --no-build-isolation\n\n# Install fish-speech\npip3 install -e .\n</code></pre>"},{"location":"en/#changelog","title":"Changelog","text":"<ul> <li>2023/12/28: Added <code>lora</code> fine-tuning support.</li> <li>2023/12/27: Add <code>gradient checkpointing</code>, <code>causual sampling</code>, and <code>flash-attn</code> support.</li> <li>2023/12/19: Updated webui and HTTP API.</li> <li>2023/12/18: Updated fine-tuning documentation and related examples.</li> <li>2023/12/17: Updated <code>text2semantic</code> model, supporting phoneme-free mode.</li> <li>2023/12/13: Beta version released, includes VQGAN model and a language model based on LLAMA (phoneme support only).</li> </ul>"},{"location":"en/#acknowledgements","title":"Acknowledgements","text":"<ul> <li>VITS2 (daniilrobnikov)</li> <li>Bert-VITS2</li> <li>GPT VITS</li> <li>MQTTS</li> <li>GPT Fast</li> <li>Transformers</li> </ul>"},{"location":"en/finetune/","title":"Fine-tuning","text":"<p>Obviously, when you opened this page, you were not satisfied with the performance of the few-shot pre-trained model. You want to fine-tune a model to improve its performance on your dataset.</p> <p><code>Fish Speech</code> consists of two modules: <code>VQGAN</code> and <code>LLAMA</code>.</p> <p>Info</p> <p>You should first conduct the following test to determine if you need to fine-tune <code>VQGAN</code>: <pre><code>python tools/vqgan/inference.py -i test.wav\n</code></pre> This test will generate a <code>fake.wav</code> file. If the timbre of this file differs from the speaker's original voice, or if the quality is not high, you need to fine-tune <code>VQGAN</code>.</p> <p>Similarly, you can refer to Inference to run <code>generate.py</code> and evaluate if the prosody meets your expectations. If it does not, then you need to fine-tune <code>LLAMA</code>.</p>"},{"location":"en/finetune/#fine-tuning-vqgan","title":"Fine-tuning VQGAN","text":""},{"location":"en/finetune/#1-prepare-the-dataset","title":"1. Prepare the Dataset","text":"<pre><code>.\n\u251c\u2500\u2500 SPK1\n\u2502   \u251c\u2500\u2500 21.15-26.44.mp3\n\u2502   \u251c\u2500\u2500 27.51-29.98.mp3\n\u2502   \u2514\u2500\u2500 30.1-32.71.mp3\n\u2514\u2500\u2500 SPK2\n    \u2514\u2500\u2500 38.79-40.85.mp3\n</code></pre> <p>You need to format your dataset as shown above and place it under <code>data/demo</code>. Audio files can have <code>.mp3</code>, <code>.wav</code>, or <code>.flac</code> extensions.</p>"},{"location":"en/finetune/#2-split-training-and-validation-sets","title":"2. Split Training and Validation Sets","text":"<pre><code>python tools/vqgan/create_train_split.py data/demo\n</code></pre> <p>This command will create <code>data/demo/vq_train_filelist.txt</code> and <code>data/demo/vq_val_filelist.txt</code> in the <code>data/demo</code> directory, to be used for training and validation respectively.</p> <p>Info</p> <p>For the VITS format, you can specify a file list using <code>--filelist xxx.list</code>. Please note that the audio files in <code>filelist</code> must also be located in the <code>data/demo</code> folder.</p>"},{"location":"en/finetune/#3-start-training","title":"3. Start Training","text":"<pre><code>python fish_speech/train.py --config-name vqgan_finetune\n</code></pre> <p>Note</p> <p>You can modify training parameters by editing <code>fish_speech/configs/vqgan_finetune.yaml</code>, but in most cases, this won't be necessary.</p>"},{"location":"en/finetune/#4-test-the-audio","title":"4. Test the Audio","text":"<pre><code>python tools/vqgan/inference.py -i test.wav --checkpoint-path results/vqgan_finetune/checkpoints/step_000010000.ckpt\n</code></pre> <p>You can review <code>fake.wav</code> to assess the fine-tuning results.</p> <p>Note</p> <p>You may also try other checkpoints. We suggest using the earliest checkpoint that meets your requirements, as they often perform better on out-of-distribution (OOD) data.</p>"},{"location":"en/finetune/#fine-tuning-llama","title":"Fine-tuning LLAMA","text":""},{"location":"en/finetune/#1-prepare-the-dataset_1","title":"1. Prepare the dataset","text":"<pre><code>.\n\u251c\u2500\u2500 SPK1\n\u2502   \u251c\u2500\u2500 21.15-26.44.lab\n\u2502   \u251c\u2500\u2500 21.15-26.44.mp3\n\u2502   \u251c\u2500\u2500 27.51-29.98.lab\n\u2502   \u251c\u2500\u2500 27.51-29.98.mp3\n\u2502   \u251c\u2500\u2500 30.1-32.71.lab\n\u2502   \u2514\u2500\u2500 30.1-32.71.mp3\n\u2514\u2500\u2500 SPK2\n    \u251c\u2500\u2500 38.79-40.85.lab\n    \u2514\u2500\u2500 38.79-40.85.mp3\n</code></pre> <p>You need to convert your dataset into the above format and place it under <code>data/demo</code>. The audio file can have the extensions <code>.mp3</code>, <code>.wav</code>, or <code>.flac</code>, and the annotation file can have the extensions <code>.lab</code> or <code>.txt</code>.</p> <p>Note</p> <p>You can modify the dataset path and mix datasets by modifying <code>fish_speech/configs/data/finetune.yaml</code>.</p>"},{"location":"en/finetune/#2-batch-extraction-of-semantic-tokens","title":"2. Batch extraction of semantic tokens","text":"<p>Make sure you have downloaded the VQGAN weights. If not, run the following command:</p> <pre><code>huggingface-cli download fishaudio/speech-lm-v1 vqgan-v1.pth --local-dir checkpoints\n</code></pre> <p>You can then run the following command to extract semantic tokens:</p> <pre><code>python tools/vqgan/extract_vq.py data/demo \\\n    --num-workers 1 --batch-size 16 \\\n    --config-name \"vqgan_pretrain\" \\\n    --checkpoint-path \"checkpoints/vqgan-v1.pth\"\n</code></pre> <p>Note</p> <p>You can adjust <code>--num-workers</code> and <code>--batch-size</code> to increase extraction speed, but please make sure not to exceed your GPU memory limit. For the VITS format, you can specify a file list using <code>--filelist xxx.list</code>.</p> <p>This command will create <code>.npy</code> files in the <code>data/demo</code> directory, as shown below:</p> <pre><code>.\n\u251c\u2500\u2500 SPK1\n\u2502   \u251c\u2500\u2500 21.15-26.44.lab\n\u2502   \u251c\u2500\u2500 21.15-26.44.mp3\n\u2502   \u251c\u2500\u2500 21.15-26.44.npy\n\u2502   \u251c\u2500\u2500 27.51-29.98.lab\n\u2502   \u251c\u2500\u2500 27.51-29.98.mp3\n\u2502   \u251c\u2500\u2500 27.51-29.98.npy\n\u2502   \u251c\u2500\u2500 30.1-32.71.lab\n\u2502   \u251c\u2500\u2500 30.1-32.71.mp3\n\u2502   \u2514\u2500\u2500 30.1-32.71.npy\n\u2514\u2500\u2500 SPK2\n    \u251c\u2500\u2500 38.79-40.85.lab\n    \u251c\u2500\u2500 38.79-40.85.mp3\n    \u2514\u2500\u2500 38.79-40.85.npy\n</code></pre>"},{"location":"en/finetune/#3-pack-the-dataset-into-protobuf","title":"3. Pack the dataset into protobuf","text":"<pre><code>python tools/llama/build_dataset.py \\\n    --config \"fish_speech/configs/data/finetune.yaml\" \\\n    --output \"data/quantized-dataset-ft.protos\"\n</code></pre> <p>After the command finishes executing, you should see the <code>quantized-dataset-ft.protos</code> file in the <code>data</code> directory.</p> <p>Info</p> <p>For the VITS format, you can specify a file list using <code>--filelist xxx.list</code>.</p>"},{"location":"en/finetune/#4-start-the-rust-data-server","title":"4. Start the Rust data server","text":"<p>Loading and shuffling the dataset is very slow and memory-consuming. Therefore, we use a Rust server to load and shuffle the data. This server is based on GRPC and can be installed using the following method:</p> <pre><code>cd data_server\ncargo build --release\n</code></pre> <p>After the compilation is complete, you can start the server using the following command:</p> <pre><code>export RUST_LOG=info # Optional, for debugging\ndata_server/target/release/data_server \\\n    --files \"data/quantized-dataset-ft.protos\" \n</code></pre> <p>Note</p> <p>You can specify multiple <code>--files</code> parameters to load multiple datasets.</p>"},{"location":"en/finetune/#5-finally-start-the-fine-tuning","title":"5. Finally, start the fine-tuning","text":"<p>Similarly, make sure you have downloaded the <code>LLAMA</code> weights. If not, run the following command:</p> <pre><code>huggingface-cli download fishaudio/speech-lm-v1 text2semantic-400m-v0.2-4k.pth --local-dir checkpoints\n</code></pre> <p>Finally, you can start the fine-tuning by running the following command: <pre><code>python fish_speech/train.py --config-name text2semantic_finetune\n</code></pre></p> <p>Info</p> <p>If you want to use lora, please use <code>--config-name text2semantic_finetune_lora</code> to start fine-tuning.</p> <p>Note</p> <p>You can modify the training parameters such as <code>batch_size</code>, <code>gradient_accumulation_steps</code>, etc. to fit your GPU memory by modifying <code>fish_speech/configs/text2semantic_finetune.yaml</code>.</p> <p>After training is complete, you can refer to the inference section, and use <code>--speaker SPK1</code> to generate speech.</p> <p>Info</p> <p>By default, the model will only learn the speaker's speech patterns and not the timbre. You still need to use prompts to ensure timbre stability. If you want to learn the timbre, you can increase the number of training steps, but this may lead to overfitting.</p>"},{"location":"en/inference/","title":"Inference","text":"<p>Inference support command line, HTTP API and web UI.</p> <p>Note</p> <p>Overall, reasoning consists of several parts:</p> <ol> <li>Encode a given 5-10 seconds of voice using VQGAN.</li> <li>Input the encoded semantic tokens and the corresponding text into the language model as an example.</li> <li>Given a new piece of text, let the model generate the corresponding semantic tokens.</li> <li>Input the generated semantic tokens into VQGAN to decode and generate the corresponding voice.</li> </ol>"},{"location":"en/inference/#command-line-inference","title":"Command Line Inference","text":"<p>Download the required <code>vqgan</code> and <code>text2semantic</code> models from our Hugging Face repository.</p> <pre><code>huggingface-cli download fishaudio/speech-lm-v1 vqgan-v1.pth --local-dir checkpoints\nhuggingface-cli download fishaudio/speech-lm-v1 text2semantic-400m-v0.2-4k.pth --local-dir checkpoints\n</code></pre>"},{"location":"en/inference/#1-generate-prompt-from-voice","title":"1. Generate prompt from voice:","text":"<p>Note</p> <p>If you plan to let the model randomly choose a voice timbre, you can skip this step.</p> <p><pre><code>python tools/vqgan/inference.py \\\n    -i \"paimon.wav\" \\\n    --checkpoint-path \"checkpoints/vqgan-v1.pth\"\n</code></pre> You should get a <code>fake.npy</code> file.</p>"},{"location":"en/inference/#2-generate-semantic-tokens-from-text","title":"2. Generate semantic tokens from text:","text":"<pre><code>python tools/llama/generate.py \\\n    --text \"The text you want to convert\" \\\n    --prompt-text \"Your reference text\" \\\n    --prompt-tokens \"fake.npy\" \\\n    --checkpoint-path \"checkpoints/text2semantic-400m-v0.2-4k.pth\" \\\n    --num-samples 2 \\\n    --compile\n</code></pre> <p>This command will create a <code>codes_N</code> file in the working directory, where N is an integer starting from 0.</p> <p>Note</p> <p>You may want to use <code>--compile</code> to fuse CUDA kernels for faster inference (~30 tokens/second -&gt; ~500 tokens/second). Correspondingly, if you do not plan to use acceleration, you can comment out the <code>--compile</code> parameter.</p> <p>Info</p> <p>For GPUs that do not support bf16, you may need to use the <code>--half</code> parameter.</p> <p>Warning</p> <p>If you are using your own fine-tuned model, please be sure to carry the <code>--speaker</code> parameter to ensure the stability of pronunciation. If you are using lora, please use <code>--config-name text2semantic_finetune_lora</code> to load the model.</p>"},{"location":"en/inference/#3-generate-vocals-from-semantic-tokens","title":"3. Generate vocals from semantic tokens:","text":"<pre><code>python tools/vqgan/inference.py \\\n    -i \"codes_0.npy\" \\\n    --checkpoint-path \"checkpoints/vqgan-v1.pth\"\n</code></pre>"},{"location":"en/inference/#http-api-inference","title":"HTTP API Inference","text":"<p>We provide a HTTP API for inference. You can use the following command to start the server:</p> <pre><code>python -m zibai tools.api_server:app --listen 127.0.0.1:8000\n</code></pre> <p>After that, you can view and test the API at http://127.0.0.1:8000/docs.  </p> <p>Generally, you need to first call PUT /v1/models/default to load the model, and then use POST /v1/models/default/invoke for inference. For specific parameters, please refer to the API documentation.</p>"},{"location":"en/inference/#webui-inference","title":"WebUI Inference","text":"<p>Before running the WebUI, you need to start the HTTP service as described above.</p> <p>Then you can start the WebUI using the following command:</p> <pre><code>python fish_speech/webui/app.py\n</code></pre> <p>Enjoy!</p>"},{"location":"en/samples/","title":"Samples","text":"<p>Note</p> <p>Due to insufficient Japanese to English training data, we first phonemicize the text and then use it for generation.</p>"},{"location":"en/samples/#chinese-sentence-1","title":"Chinese Sentence 1","text":"<pre><code>\u4eba\u95f4\u706f\u706b\u5012\u6620\u6e56\u4e2d\uff0c\u5979\u7684\u6e34\u671b\u8ba9\u9759\u6c34\u6cdb\u8d77\u6d9f\u6f2a\u3002\u82e5\u4ee3\u4ef7\u53ea\u662f\u5b64\u72ec\uff0c\u90a3\u5c31\u8ba9\u8fd9\u4efd\u613f\u671b\u8086\u610f\u6d41\u6dcc\u3002\n\u6d41\u5165\u5979\u6240\u6ce8\u89c6\u7684\u4e16\u95f4\uff0c\u4e5f\u6d41\u5165\u5979\u5982\u6e56\u6c34\u822c\u6f84\u6f88\u7684\u76ee\u5149\u3002\n</code></pre> Speaker Input Audio Synthesized Audio Nahida (Genshin Impact) Zhongli (Genshin Impact) Furina (Genshin Impact) Random Speaker 1  -  Random Speaker 2  -"},{"location":"en/samples/#chinese-sentence-2-long-sentence","title":"Chinese Sentence 2 (Long Sentence)","text":"<pre><code>\u4f60\u4eec\u8fd9\u4e2a\u662f\u4ec0\u4e48\u7fa4\u554a\uff0c\u4f60\u4eec\u8fd9\u662f\u5bb3\u4eba\u4e0d\u6d45\u554a\u4f60\u4eec\u8fd9\u4e2a\u7fa4\uff01\u8c01\u662f\u7fa4\u4e3b\uff0c\u51fa\u6765\uff01\u771f\u7684\u592a\u8fc7\u5206\u4e86\u3002\u4f60\u4eec\u641e\u8fd9\u4e2a\u7fa4\u5e72\u4ec0\u4e48\uff1f\n\u6211\u513f\u5b50\u6bcf\u4e00\u79d1\u7684\u6210\u7ee9\u90fd\u4e0d\u8fc7\u90a3\u4e2a\u5e73\u5747\u5206\u5450\uff0c\u4ed6\u73b0\u5728\u521d\u4e8c\uff0c\u4f60\u53eb\u6211\u513f\u5b50\u600e\u4e48\u529e\u554a\uff1f\u4ed6\u73b0\u5728\u8fd8\u4e0d\u5230\u9ad8\u4e2d\u554a\uff1f\n\u4f60\u4eec\u5bb3\u6b7b\u6211\u513f\u5b50\u4e86\uff01\u5feb\u70b9\u51fa\u6765\u4f60\u8fd9\u4e2a\u7fa4\u4e3b\uff01\u518d\u8fd9\u6837\u6211\u53bb\u62a5\u8b66\u4e86\u554a\uff01\u6211\u8ddf\u4f60\u4eec\u8bf4\u4f60\u4eec\u8fd9\u4e00\u5e2e\u4eba\u554a\uff0c\u4e00\u5929\u5230\u665a\u554a\uff0c\n\u641e\u8fd9\u4e9b\u4ec0\u4e48\u6e38\u620f\u554a\uff0c\u52a8\u6f2b\u554a\uff0c\u4f1a\u5bb3\u6b7b\u4f60\u4eec\u7684\uff0c\u4f60\u4eec\u6ca1\u6709\u524d\u9014\u6211\u8ddf\u4f60\u8bf4\u3002\u4f60\u4eec\u8fd9\u4e5d\u767e\u591a\u4e2a\u4eba\uff0c\u597d\u597d\u5b66\u4e60\u4e0d\u597d\u5417\uff1f\n\u4e00\u5929\u5230\u665a\u5728\u4e0a\u7f51\u3002\u6709\u4ec0\u4e48\u610f\u601d\u554a\uff1f\u9ebb\u70e6\u4f60\u91cd\u89c6\u4e00\u4e0b\u4f60\u4eec\u7684\u751f\u6d3b\u7684\u76ee\u6807\u554a\uff1f\u6709\u4e00\u70b9\u5b66\u4e60\u76ee\u6807\u884c\u4e0d\u884c\uff1f\u4e00\u5929\u5230\u665a\u4e0a\u7f51\u662f\u4e0d\u662f\u4eba\u554a\uff1f\n</code></pre> Speaker Input Audio Synthesized Audio Nahida (Genshin Impact) Seiki (Honkai: StarRail)"},{"location":"en/samples/#english-sentence","title":"English Sentence","text":"<pre><code>In the realm of advanced technology, the evolution of artificial intelligence stands as a \nmonumental achievement. This dynamic field, constantly pushing the boundaries of what \nmachines can do, has seen rapid growth and innovation. From deciphering complex data \npatterns to driving cars autonomously, AI's applications are vast and diverse.\n</code></pre> Speaker Input Audio Synthesized Audio Speaker 200 (LibriTTS) Random Speaker 1  -  Random Speaker 2  -"},{"location":"en/samples/#japanese-sentence","title":"Japanese Sentence","text":"<pre><code>\u5148\u9032\u6280\u8853\u306e\u9818\u57df\u306b\u304a\u3044\u3066\u3001\u4eba\u5de5\u77e5\u80fd\u306e\u9032\u5316\u306f\u753b\u671f\u7684\u306a\u6210\u679c\u3068\u3057\u3066\u7acb\u3063\u3066\u3044\u307e\u3059\u3002\u5e38\u306b\u6a5f\u68b0\u304c\u3067\u304d\u308b\u3053\u3068\u306e\u9650\u754c\u3092\n\u62bc\u3057\u5e83\u3052\u3066\u3044\u308b\u3053\u306e\u30c0\u30a4\u30ca\u30df\u30c3\u30af\u306a\u5206\u91ce\u306f\u3001\u6025\u901f\u306a\u6210\u9577\u3068\u9769\u65b0\u3092\u898b\u305b\u3066\u3044\u307e\u3059\u3002\u8907\u96d1\u306a\u30c7\u30fc\u30bf\u30d1\u30bf\u30fc\u30f3\u306e\u89e3\u8aad\u304b\n\u3089\u81ea\u52d5\u904b\u8ee2\u8eca\u306e\u64cd\u7e26\u307e\u3067\u3001AI\u306e\u5fdc\u7528\u306f\u5e83\u7bc4\u56f2\u306b\u53ca\u3073\u307e\u3059\u3002\n</code></pre> Speaker Input Audio Synthesized Audio Random Speaker 1  -  Random Speaker 2  -"},{"location":"","title":"\u4ecb\u7ecd","text":"<p>Warning</p> <p>\u6211\u4eec\u4e0d\u5bf9\u4ee3\u7801\u5e93\u7684\u4efb\u4f55\u975e\u6cd5\u4f7f\u7528\u627f\u62c5\u4efb\u4f55\u8d23\u4efb. \u8bf7\u53c2\u9605\u60a8\u5f53\u5730\u5173\u4e8e DMCA (\u6570\u5b57\u5343\u5e74\u6cd5\u6848) \u548c\u5176\u4ed6\u76f8\u5173\u6cd5\u5f8b\u6cd5\u89c4.</p> <p>\u6b64\u4ee3\u7801\u5e93\u6839\u636e <code>BSD-3-Clause</code> \u8bb8\u53ef\u8bc1\u53d1\u5e03, \u6240\u6709\u6a21\u578b\u6839\u636e CC-BY-NC-SA-4.0 \u8bb8\u53ef\u8bc1\u53d1\u5e03.</p> <p> </p>"},{"location":"#_2","title":"\u8981\u6c42","text":"<ul> <li>GPU\u5185\u5b58: 2GB (\u7528\u4e8e\u63a8\u7406), 16GB (\u7528\u4e8e\u5fae\u8c03)</li> <li>\u7cfb\u7edf: Linux (\u5168\u90e8\u529f\u80fd), Windows (\u4ec5\u63a8\u7406, \u4e0d\u652f\u6301 <code>flash-attn</code>, \u4e0d\u652f\u6301 <code>torch.compile</code>)</li> </ul> <p>\u56e0\u6b64, \u6211\u4eec\u5f3a\u70c8\u5efa\u8bae Windows \u7528\u6237\u4f7f\u7528 WSL2 \u6216 docker \u6765\u8fd0\u884c\u4ee3\u7801\u5e93.</p>"},{"location":"#_3","title":"\u8bbe\u7f6e","text":"<pre><code># \u521b\u5efa\u4e00\u4e2a python 3.10 \u865a\u62df\u73af\u5883, \u4f60\u4e5f\u53ef\u4ee5\u7528 virtualenv\nconda create -n fish-speech python=3.10\nconda activate fish-speech\n\n# \u5b89\u88c5 pytorch nightly \u7248\u672c\npip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121\n\n# \u5b89\u88c5 flash-attn (\u9002\u7528\u4e8elinux)\npip3 install ninja &amp;&amp; MAX_JOBS=4 pip3 install flash-attn --no-build-isolation\n\n# \u5b89\u88c5 fish-speech\npip3 install -e .\n</code></pre>"},{"location":"#_4","title":"\u66f4\u65b0\u65e5\u5fd7","text":"<ul> <li>2023/12/28: \u6dfb\u52a0\u4e86 <code>lora</code> \u5fae\u8c03\u652f\u6301.</li> <li>2023/12/27: \u6dfb\u52a0\u4e86 <code>gradient checkpointing</code>, <code>causual sampling</code> \u548c <code>flash-attn</code> \u652f\u6301.</li> <li>2023/12/19: \u66f4\u65b0\u4e86 Webui \u548c HTTP API.</li> <li>2023/12/18: \u66f4\u65b0\u4e86\u5fae\u8c03\u6587\u6863\u548c\u76f8\u5173\u4f8b\u5b50.</li> <li>2023/12/17: \u66f4\u65b0\u4e86 <code>text2semantic</code> \u6a21\u578b, \u652f\u6301\u65e0\u97f3\u7d20\u6a21\u5f0f.</li> <li>2023/12/13: \u6d4b\u8bd5\u7248\u53d1\u5e03, \u5305\u542b VQGAN \u6a21\u578b\u548c\u4e00\u4e2a\u57fa\u4e8e LLAMA \u7684\u8bed\u8a00\u6a21\u578b (\u53ea\u652f\u6301\u97f3\u7d20).</li> </ul>"},{"location":"#_5","title":"\u81f4\u8c22","text":"<ul> <li>VITS2 (daniilrobnikov)</li> <li>Bert-VITS2</li> <li>GPT VITS</li> <li>MQTTS</li> <li>GPT Fast</li> <li>Transformers</li> </ul>"},{"location":"finetune/","title":"\u5fae\u8c03","text":"<p>\u663e\u7136, \u5f53\u4f60\u6253\u5f00\u8fd9\u4e2a\u9875\u9762\u7684\u65f6\u5019, \u4f60\u5df2\u7ecf\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b few-shot \u7684\u6548\u679c\u4e0d\u7b97\u6ee1\u610f. \u4f60\u60f3\u8981\u5fae\u8c03\u4e00\u4e2a\u6a21\u578b, \u4f7f\u5f97\u5b83\u5728\u4f60\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u597d.  </p> <p><code>Fish Speech</code> \u7531\u4e24\u4e2a\u6a21\u5757\u7ec4\u6210: <code>VQGAN</code> \u548c <code>LLAMA</code>. </p> <p>Info</p> <p>\u4f60\u5e94\u8be5\u5148\u8fdb\u884c\u5982\u4e0b\u6d4b\u8bd5\u6765\u5224\u65ad\u4f60\u662f\u5426\u9700\u8981\u5fae\u8c03 <code>VQGAN</code>: <pre><code>python tools/vqgan/inference.py -i test.wav\n</code></pre> \u8be5\u6d4b\u8bd5\u4f1a\u751f\u6210\u4e00\u4e2a <code>fake.wav</code> \u6587\u4ef6, \u5982\u679c\u8be5\u6587\u4ef6\u7684\u97f3\u8272\u548c\u8bf4\u8bdd\u4eba\u7684\u97f3\u8272\u4e0d\u540c, \u6216\u8005\u8d28\u91cf\u4e0d\u9ad8, \u4f60\u9700\u8981\u5fae\u8c03 <code>VQGAN</code>.</p> <p>\u76f8\u5e94\u7684, \u4f60\u53ef\u4ee5\u53c2\u8003 \u63a8\u7406 \u6765\u8fd0\u884c <code>generate.py</code>, \u5224\u65ad\u97f5\u5f8b\u662f\u5426\u6ee1\u610f, \u5982\u679c\u4e0d\u6ee1\u610f, \u5219\u9700\u8981\u5fae\u8c03 <code>LLAMA</code>.</p>"},{"location":"finetune/#vqgan","title":"VQGAN \u5fae\u8c03","text":""},{"location":"finetune/#1","title":"1. \u51c6\u5907\u6570\u636e\u96c6","text":"<pre><code>.\n\u251c\u2500\u2500 SPK1\n\u2502   \u251c\u2500\u2500 21.15-26.44.mp3\n\u2502   \u251c\u2500\u2500 27.51-29.98.mp3\n\u2502   \u2514\u2500\u2500 30.1-32.71.mp3\n\u2514\u2500\u2500 SPK2\n    \u2514\u2500\u2500 38.79-40.85.mp3\n</code></pre> <p>\u4f60\u9700\u8981\u5c06\u6570\u636e\u96c6\u8f6c\u4e3a\u4ee5\u4e0a\u683c\u5f0f, \u5e76\u653e\u5230 <code>data/demo</code> \u4e0b, \u97f3\u9891\u540e\u7f00\u53ef\u4ee5\u4e3a <code>.mp3</code>, <code>.wav</code> \u6216 <code>.flac</code>.</p>"},{"location":"finetune/#2","title":"2. \u5206\u5272\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6","text":"<pre><code>python tools/vqgan/create_train_split.py data/demo\n</code></pre> <p>\u8be5\u547d\u4ee4\u4f1a\u5728 <code>data/demo</code> \u76ee\u5f55\u4e0b\u521b\u5efa <code>data/demo/vq_train_filelist.txt</code> \u548c <code>data/demo/vq_val_filelist.txt</code> \u6587\u4ef6, \u5206\u522b\u7528\u4e8e\u8bad\u7ec3\u548c\u9a8c\u8bc1.  </p> <p>Info</p> <p>\u5bf9\u4e8e VITS \u683c\u5f0f, \u4f60\u53ef\u4ee5\u4f7f\u7528 <code>--filelist xxx.list</code> \u6765\u6307\u5b9a\u6587\u4ef6\u5217\u8868. \u8bf7\u6ce8\u610f, <code>filelist</code> \u6240\u6307\u5411\u7684\u97f3\u9891\u6587\u4ef6\u5fc5\u987b\u4e5f\u4f4d\u4e8e <code>data/demo</code> \u6587\u4ef6\u5939\u4e0b.</p>"},{"location":"finetune/#3","title":"3. \u542f\u52a8\u8bad\u7ec3","text":"<pre><code>python fish_speech/train.py --config-name vqgan_finetune\n</code></pre> <p>Note</p> <p>\u4f60\u53ef\u4ee5\u901a\u8fc7\u4fee\u6539 <code>fish_speech/configs/vqgan_finetune.yaml</code> \u6765\u4fee\u6539\u8bad\u7ec3\u53c2\u6570, \u4f46\u5927\u90e8\u5206\u60c5\u51b5\u4e0b, \u4f60\u4e0d\u9700\u8981\u8fd9\u4e48\u505a.</p>"},{"location":"finetune/#4","title":"4. \u6d4b\u8bd5\u97f3\u9891","text":"<pre><code>python tools/vqgan/inference.py -i test.wav --checkpoint-path results/vqgan_finetune/checkpoints/step_000010000.ckpt\n</code></pre> <p>\u4f60\u53ef\u4ee5\u67e5\u770b <code>fake.wav</code> \u6765\u5224\u65ad\u5fae\u8c03\u6548\u679c.</p> <p>Note</p> <p>\u4f60\u4e5f\u53ef\u4ee5\u5c1d\u8bd5\u5176\u4ed6\u7684 checkpoint, \u6211\u4eec\u5efa\u8bae\u4f60\u4f7f\u7528\u6700\u65e9\u7684\u6ee1\u8db3\u4f60\u8981\u6c42\u7684 checkpoint, \u4ed6\u4eec\u901a\u5e38\u5728 OOD \u4e0a\u8868\u73b0\u66f4\u597d.</p>"},{"location":"finetune/#llama","title":"LLAMA \u5fae\u8c03","text":""},{"location":"finetune/#1_1","title":"1. \u51c6\u5907\u6570\u636e\u96c6","text":"<pre><code>.\n\u251c\u2500\u2500 SPK1\n\u2502   \u251c\u2500\u2500 21.15-26.44.lab\n\u2502   \u251c\u2500\u2500 21.15-26.44.mp3\n\u2502   \u251c\u2500\u2500 27.51-29.98.lab\n\u2502   \u251c\u2500\u2500 27.51-29.98.mp3\n\u2502   \u251c\u2500\u2500 30.1-32.71.lab\n\u2502   \u2514\u2500\u2500 30.1-32.71.mp3\n\u2514\u2500\u2500 SPK2\n    \u251c\u2500\u2500 38.79-40.85.lab\n    \u2514\u2500\u2500 38.79-40.85.mp3\n</code></pre> <p>\u4f60\u9700\u8981\u5c06\u6570\u636e\u96c6\u8f6c\u4e3a\u4ee5\u4e0a\u683c\u5f0f, \u5e76\u653e\u5230 <code>data/demo</code> \u4e0b, \u97f3\u9891\u540e\u7f00\u53ef\u4ee5\u4e3a <code>.mp3</code>, <code>.wav</code> \u6216 <code>.flac</code>, \u6807\u6ce8\u6587\u4ef6\u540e\u7f00\u53ef\u4ee5\u4e3a <code>.lab</code> \u6216 <code>.txt</code>.</p> <p>Note</p> <p>\u4f60\u53ef\u4ee5\u901a\u8fc7\u4fee\u6539 <code>fish_speech/configs/data/finetune.yaml</code> \u6765\u4fee\u6539\u6570\u636e\u96c6\u8def\u5f84, \u4ee5\u53ca\u6df7\u5408\u6570\u636e\u96c6.</p> <p>Warning</p> <p>\u5efa\u8bae\u5148\u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u54cd\u5ea6\u5339\u914d, \u4f60\u53ef\u4ee5\u4f7f\u7528 fish-audio-preprocess \u6765\u5b8c\u6210\u8fd9\u4e00\u6b65\u9aa4.  <pre><code>fap loudness-norm demo-raw demo --clean\n</code></pre></p>"},{"location":"finetune/#2-token","title":"2. \u6279\u91cf\u63d0\u53d6\u8bed\u4e49 token","text":"<p>\u786e\u4fdd\u4f60\u5df2\u7ecf\u4e0b\u8f7d\u4e86 vqgan \u6743\u91cd, \u5982\u679c\u6ca1\u6709, \u8bf7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4:</p> <pre><code>huggingface-cli download fishaudio/speech-lm-v1 vqgan-v1.pth --local-dir checkpoints\n</code></pre> <p>\u5bf9\u4e8e\u4e2d\u56fd\u5927\u9646\u7528\u6237, \u53ef\u4f7f\u7528 mirror \u4e0b\u8f7d.</p> <pre><code>HF_ENDPOINT=https://hf-mirror.com huggingface-cli download fishaudio/speech-lm-v1 vqgan-v1.pth --local-dir checkpoints\n</code></pre> <p>\u968f\u540e\u53ef\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u63d0\u53d6\u8bed\u4e49 token:</p> <pre><code>python tools/vqgan/extract_vq.py data/demo \\\n    --num-workers 1 --batch-size 16 \\\n    --config-name \"vqgan_pretrain\" \\\n    --checkpoint-path \"checkpoints/vqgan-v1.pth\"\n</code></pre> <p>Note</p> <p>\u4f60\u53ef\u4ee5\u8c03\u6574 <code>--num-workers</code> \u548c <code>--batch-size</code> \u6765\u63d0\u9ad8\u63d0\u53d6\u901f\u5ea6, \u4f46\u662f\u8bf7\u6ce8\u610f\u4e0d\u8981\u8d85\u8fc7\u4f60\u7684\u663e\u5b58\u9650\u5236. \u5bf9\u4e8e VITS \u683c\u5f0f, \u4f60\u53ef\u4ee5\u4f7f\u7528 <code>--filelist xxx.list</code> \u6765\u6307\u5b9a\u6587\u4ef6\u5217\u8868.</p> <p>\u8be5\u547d\u4ee4\u4f1a\u5728 <code>data/demo</code> \u76ee\u5f55\u4e0b\u521b\u5efa <code>.npy</code> \u6587\u4ef6, \u5982\u4e0b\u6240\u793a:</p> <pre><code>.\n\u251c\u2500\u2500 SPK1\n\u2502   \u251c\u2500\u2500 21.15-26.44.lab\n\u2502   \u251c\u2500\u2500 21.15-26.44.mp3\n\u2502   \u251c\u2500\u2500 21.15-26.44.npy\n\u2502   \u251c\u2500\u2500 27.51-29.98.lab\n\u2502   \u251c\u2500\u2500 27.51-29.98.mp3\n\u2502   \u251c\u2500\u2500 27.51-29.98.npy\n\u2502   \u251c\u2500\u2500 30.1-32.71.lab\n\u2502   \u251c\u2500\u2500 30.1-32.71.mp3\n\u2502   \u2514\u2500\u2500 30.1-32.71.npy\n\u2514\u2500\u2500 SPK2\n    \u251c\u2500\u2500 38.79-40.85.lab\n    \u251c\u2500\u2500 38.79-40.85.mp3\n    \u2514\u2500\u2500 38.79-40.85.npy\n</code></pre>"},{"location":"finetune/#3-protobuf","title":"3. \u6253\u5305\u6570\u636e\u96c6\u4e3a protobuf","text":"<pre><code>python tools/llama/build_dataset.py \\\n    --config \"fish_speech/configs/data/finetune.yaml\" \\\n    --output \"data/quantized-dataset-ft.protos\" \\\n    --num-workers 16\n</code></pre> <p>\u547d\u4ee4\u6267\u884c\u5b8c\u6bd5\u540e, \u4f60\u5e94\u8be5\u80fd\u5728 <code>data</code> \u76ee\u5f55\u4e0b\u770b\u5230 <code>quantized-dataset-ft.protos</code> \u6587\u4ef6.</p> <p>Note</p> <p>\u5bf9\u4e8e VITS \u683c\u5f0f, \u4f60\u53ef\u4ee5\u4f7f\u7528 <code>--filelist xxx.list</code> \u6765\u6307\u5b9a\u6587\u4ef6\u5217\u8868.</p>"},{"location":"finetune/#4-rust","title":"4. \u542f\u52a8 Rust \u6570\u636e\u670d\u52a1\u5668","text":"<p>\u7531\u4e8e\u52a0\u8f7d\u548c\u6253\u4e71\u6570\u636e\u96c6\u975e\u5e38\u7f13\u6162\u4e14\u5360\u7528\u5185\u5b58, \u56e0\u6b64\u6211\u4eec\u4f7f\u7528 rust \u670d\u52a1\u5668\u6765\u52a0\u8f7d\u548c\u6253\u4e71\u6570\u636e. \u8be5\u670d\u52a1\u5668\u57fa\u4e8e GRPC, \u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u5b89\u88c5:</p> <pre><code>cd data_server\ncargo build --release\n</code></pre> <p>\u7f16\u8bd1\u5b8c\u6210\u540e\u4f60\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u6765\u542f\u52a8\u670d\u52a1\u5668:</p> <pre><code>export RUST_LOG=info # \u53ef\u9009, \u7528\u4e8e\u8c03\u8bd5\ndata_server/target/release/data_server \\\n    --files \"data/quantized-dataset-ft.protos\" \n</code></pre> <p>Note</p> <p>\u4f60\u53ef\u4ee5\u6307\u5b9a\u591a\u4e2a <code>--files</code> \u53c2\u6570\u6765\u52a0\u8f7d\u591a\u4e2a\u6570\u636e\u96c6.</p>"},{"location":"finetune/#5","title":"5. \u6700\u540e, \u542f\u52a8\u5fae\u8c03","text":"<p>\u540c\u6837\u7684, \u8bf7\u786e\u4fdd\u4f60\u5df2\u7ecf\u4e0b\u8f7d\u4e86 <code>LLAMA</code> \u6743\u91cd, \u5982\u679c\u6ca1\u6709, \u8bf7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4:</p> <pre><code>huggingface-cli download fishaudio/speech-lm-v1 text2semantic-400m-v0.2-4k.pth --local-dir checkpoints\n</code></pre> <p>\u5bf9\u4e8e\u4e2d\u56fd\u5927\u9646\u7528\u6237, \u53ef\u4f7f\u7528 mirror \u4e0b\u8f7d.</p> <pre><code>HF_ENDPOINT=https://hf-mirror.com huggingface-cli download fishaudio/speech-lm-v1 text2semantic-400m-v0.2-4k.pth --local-dir checkpoints\n</code></pre> <p>\u6700\u540e, \u4f60\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u542f\u52a8\u5fae\u8c03:</p> <pre><code>python fish_speech/train.py --config-name text2semantic_finetune\n</code></pre> <p>Note</p> <p>\u5982\u679c\u4f60\u60f3\u4f7f\u7528 lora, \u8bf7\u4f7f\u7528 <code>--config-name text2semantic_finetune_lora</code> \u6765\u542f\u52a8\u5fae\u8c03.</p> <p>Note</p> <p>\u4f60\u53ef\u4ee5\u901a\u8fc7\u4fee\u6539 <code>fish_speech/configs/text2semantic_finetune.yaml</code> \u6765\u4fee\u6539\u8bad\u7ec3\u53c2\u6570\u5982 <code>batch_size</code>, <code>gradient_accumulation_steps</code> \u7b49, \u6765\u9002\u5e94\u4f60\u7684\u663e\u5b58.</p> <p>\u8bad\u7ec3\u7ed3\u675f\u540e, \u4f60\u53ef\u4ee5\u53c2\u8003 \u63a8\u7406 \u90e8\u5206, \u5e76\u643a\u5e26 <code>--speaker SPK1</code> \u53c2\u6570\u6765\u6d4b\u8bd5\u4f60\u7684\u6a21\u578b.</p> <p>Info</p> <p>\u9ed8\u8ba4\u914d\u7f6e\u4e0b, \u57fa\u672c\u53ea\u4f1a\u5b66\u5230\u8bf4\u8bdd\u4eba\u7684\u53d1\u97f3\u65b9\u5f0f, \u800c\u4e0d\u5305\u542b\u97f3\u8272, \u4f60\u4f9d\u7136\u9700\u8981\u4f7f\u7528 prompt \u6765\u4fdd\u8bc1\u97f3\u8272\u7684\u7a33\u5b9a\u6027. \u5982\u679c\u4f60\u60f3\u8981\u5b66\u5230\u97f3\u8272, \u8bf7\u5c06\u8bad\u7ec3\u6b65\u6570\u8c03\u5927, \u4f46\u8fd9\u6709\u53ef\u80fd\u4f1a\u5bfc\u81f4\u8fc7\u62df\u5408.</p>"},{"location":"inference/","title":"\u63a8\u7406","text":"<p>\u63a8\u7406\u652f\u6301\u547d\u4ee4\u884c, http api, \u4ee5\u53ca webui \u4e09\u79cd\u65b9\u5f0f.  </p> <p>Note</p> <p>\u603b\u7684\u6765\u8bf4, \u63a8\u7406\u5206\u4e3a\u51e0\u4e2a\u90e8\u5206:  </p> <ol> <li>\u7ed9\u5b9a\u4e00\u6bb5 5-10 \u79d2\u7684\u8bed\u97f3, \u5c06\u5b83\u7528 VQGAN \u7f16\u7801.  </li> <li>\u5c06\u7f16\u7801\u540e\u7684\u8bed\u4e49 token \u548c\u5bf9\u5e94\u6587\u672c\u8f93\u5165\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u4f8b\u5b50.  </li> <li>\u7ed9\u5b9a\u4e00\u6bb5\u65b0\u6587\u672c, \u8ba9\u6a21\u578b\u751f\u6210\u5bf9\u5e94\u7684\u8bed\u4e49 token.  </li> <li>\u5c06\u751f\u6210\u7684\u8bed\u4e49 token \u8f93\u5165 VQGAN \u89e3\u7801, \u751f\u6210\u5bf9\u5e94\u7684\u8bed\u97f3.  </li> </ol>"},{"location":"inference/#_2","title":"\u547d\u4ee4\u884c\u63a8\u7406","text":"<p>\u4ece\u6211\u4eec\u7684 huggingface \u4ed3\u5e93\u4e0b\u8f7d\u6240\u9700\u7684 <code>vqgan</code> \u548c <code>text2semantic</code> \u6a21\u578b\u3002</p> <p><pre><code>huggingface-cli download fishaudio/speech-lm-v1 vqgan-v1.pth --local-dir checkpoints\nhuggingface-cli download fishaudio/speech-lm-v1 text2semantic-400m-v0.2-4k.pth --local-dir checkpoints\n</code></pre> \u5bf9\u4e8e\u4e2d\u56fd\u5927\u9646\u7528\u6237\uff0c\u53ef\u4f7f\u7528mirror\u4e0b\u8f7d\u3002 <pre><code>HF_ENDPOINT=https://hf-mirror.com huggingface-cli download fishaudio/speech-lm-v1 vqgan-v1.pth --local-dir checkpoints\nHF_ENDPOINT=https://hf-mirror.com huggingface-cli download fishaudio/speech-lm-v1 text2semantic-400m-v0.2-4k.pth --local-dir checkpoints\n</code></pre></p>"},{"location":"inference/#1-prompt","title":"1. \u4ece\u8bed\u97f3\u751f\u6210 prompt:","text":"<p>Note</p> <p>\u5982\u679c\u4f60\u6253\u7b97\u8ba9\u6a21\u578b\u968f\u673a\u9009\u62e9\u97f3\u8272, \u4f60\u53ef\u4ee5\u8df3\u8fc7\u8fd9\u4e00\u6b65.</p> <p><pre><code>python tools/vqgan/inference.py \\\n    -i \"paimon.wav\" \\\n    --checkpoint-path \"checkpoints/vqgan-v1.pth\"\n</code></pre> \u4f60\u5e94\u8be5\u80fd\u5f97\u5230\u4e00\u4e2a <code>fake.npy</code> \u6587\u4ef6.</p>"},{"location":"inference/#2-token","title":"2. \u4ece\u6587\u672c\u751f\u6210\u8bed\u4e49 token:","text":"<pre><code>python tools/llama/generate.py \\\n    --text \"\u8981\u8f6c\u6362\u7684\u6587\u672c\" \\\n    --prompt-text \"\u4f60\u7684\u53c2\u8003\u6587\u672c\" \\\n    --prompt-tokens \"fake.npy\" \\\n    --checkpoint-path \"checkpoints/text2semantic-400m-v0.2-4k.pth\" \\\n    --num-samples 2 \\\n    --compile\n</code></pre> <p>\u8be5\u547d\u4ee4\u4f1a\u5728\u5de5\u4f5c\u76ee\u5f55\u4e0b\u521b\u5efa <code>codes_N</code> \u6587\u4ef6, \u5176\u4e2d N \u662f\u4ece 0 \u5f00\u59cb\u7684\u6574\u6570.</p> <p>Note</p> <p>\u60a8\u53ef\u80fd\u5e0c\u671b\u4f7f\u7528 <code>--compile</code> \u6765\u878d\u5408 cuda \u5185\u6838\u4ee5\u5b9e\u73b0\u66f4\u5feb\u7684\u63a8\u7406 (~30 \u4e2a token/\u79d2 -&gt; ~500 \u4e2a token/\u79d2). \u5bf9\u5e94\u7684, \u5982\u679c\u4f60\u4e0d\u6253\u7b97\u4f7f\u7528\u52a0\u901f, \u4f60\u53ef\u4ee5\u6ce8\u91ca\u6389 <code>--compile</code> \u53c2\u6570.</p> <p>Info</p> <p>\u5bf9\u4e8e\u4e0d\u652f\u6301 bf16 \u7684 GPU, \u4f60\u53ef\u80fd\u9700\u8981\u4f7f\u7528 <code>--half</code> \u53c2\u6570.</p> <p>Warning</p> <p>\u5982\u679c\u4f60\u5728\u4f7f\u7528\u81ea\u5df1\u5fae\u8c03\u7684\u6a21\u578b, \u8bf7\u52a1\u5fc5\u643a\u5e26 <code>--speaker</code> \u53c2\u6570\u6765\u4fdd\u8bc1\u53d1\u97f3\u7684\u7a33\u5b9a\u6027. \u5982\u679c\u4f60\u4f7f\u7528\u4e86 lora, \u8bf7\u4f7f\u7528 <code>--config-name text2semantic_finetune_lora</code> \u6765\u52a0\u8f7d\u6a21\u578b.</p>"},{"location":"inference/#3-token","title":"3. \u4ece\u8bed\u4e49 token \u751f\u6210\u4eba\u58f0:","text":"<pre><code>python tools/vqgan/inference.py \\\n    -i \"codes_0.npy\" \\\n    --checkpoint-path \"checkpoints/vqgan-v1.pth\"\n</code></pre>"},{"location":"inference/#http-api","title":"HTTP API \u63a8\u7406","text":"<p>\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u542f\u52a8 HTTP \u670d\u52a1:</p> <pre><code>python -m zibai tools.api_server:app --listen 127.0.0.1:8000\n# \u63a8\u8350\u4e2d\u56fd\u5927\u9646\u7528\u6237\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u542f\u52a8 HTTP \u670d\u52a1:\nHF_ENDPOINT=https://hf-mirror.com python -m zibai tools.api_server:app --listen 127.0.0.1:8000\n</code></pre> <p>\u968f\u540e, \u4f60\u53ef\u4ee5\u5728 <code>http://127.0.0.1:8000/docs</code> \u4e2d\u67e5\u770b\u5e76\u6d4b\u8bd5 API. \u4e00\u822c\u6765\u8bf4, \u4f60\u9700\u8981\u5148\u8c03\u7528 <code>PUT /v1/models/default</code> \u6765\u52a0\u8f7d\u6a21\u578b, \u7136\u540e\u8c03\u7528 <code>POST /v1/models/default/invoke</code> \u6765\u8fdb\u884c\u63a8\u7406. \u5177\u4f53\u7684\u53c2\u6570\u8bf7\u53c2\u8003 API \u6587\u6863.</p>"},{"location":"inference/#webui","title":"WebUI \u63a8\u7406","text":"<p>\u5728\u8fd0\u884c WebUI \u4e4b\u524d, \u4f60\u9700\u8981\u5148\u542f\u52a8 HTTP \u670d\u52a1, \u5982\u4e0a\u6240\u8ff0.</p> <p>\u968f\u540e\u4f60\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u6765\u542f\u52a8 WebUI:</p> <pre><code>python fish_speech/webui/app.py\n</code></pre> <p>\u6216\u9644\u5e26\u53c2\u6570\u6765\u542f\u52a8 WebUI:</p> <pre><code># \u4ee5\u4e34\u65f6\u73af\u5883\u53d8\u91cf\u7684\u65b9\u5f0f\u542f\u52a8:\nGRADIO_SERVER_NAME=127.0.0.1 GRADIO_SERVER_PORT=7860 python fish_speech/webui/app.py\n</code></pre> <p>\u795d\u5927\u5bb6\u73a9\u5f97\u5f00\u5fc3!</p>"},{"location":"samples/","title":"\u4f8b\u5b50","text":"<p>Note</p> <p>\u7531\u4e8e\u65e5\u82f1\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3, \u6211\u4eec\u5148\u5c06\u6587\u672c\u97f3\u7d20\u5316, \u518d\u7528\u4e8e\u751f\u6210.</p>"},{"location":"samples/#1","title":"\u4e2d\u6587\u53e5\u5b50 1","text":"<pre><code>\u4eba\u95f4\u706f\u706b\u5012\u6620\u6e56\u4e2d\uff0c\u5979\u7684\u6e34\u671b\u8ba9\u9759\u6c34\u6cdb\u8d77\u6d9f\u6f2a\u3002\u82e5\u4ee3\u4ef7\u53ea\u662f\u5b64\u72ec\uff0c\u90a3\u5c31\u8ba9\u8fd9\u4efd\u613f\u671b\u8086\u610f\u6d41\u6dcc\u3002\n\u6d41\u5165\u5979\u6240\u6ce8\u89c6\u7684\u4e16\u95f4\uff0c\u4e5f\u6d41\u5165\u5979\u5982\u6e56\u6c34\u822c\u6f84\u6f88\u7684\u76ee\u5149\u3002\n</code></pre> \u8bf4\u8bdd\u4eba \u8f93\u5165\u97f3\u9891 \u5408\u6210\u97f3\u9891 \u7eb3\u897f\u59b2 (\u539f\u795e) \u949f\u79bb (\u539f\u795e) \u8299\u5b81\u5a1c (\u539f\u795e) \u968f\u673a\u8bf4\u8bdd\u4eba 1  -  \u968f\u673a\u8bf4\u8bdd\u4eba 2  -"},{"location":"samples/#2","title":"\u4e2d\u6587\u53e5\u5b50 2 (\u957f\u53e5)","text":"<pre><code>\u4f60\u4eec\u8fd9\u4e2a\u662f\u4ec0\u4e48\u7fa4\u554a\uff0c\u4f60\u4eec\u8fd9\u662f\u5bb3\u4eba\u4e0d\u6d45\u554a\u4f60\u4eec\u8fd9\u4e2a\u7fa4\uff01\u8c01\u662f\u7fa4\u4e3b\uff0c\u51fa\u6765\uff01\u771f\u7684\u592a\u8fc7\u5206\u4e86\u3002\u4f60\u4eec\u641e\u8fd9\u4e2a\u7fa4\u5e72\u4ec0\u4e48\uff1f\n\u6211\u513f\u5b50\u6bcf\u4e00\u79d1\u7684\u6210\u7ee9\u90fd\u4e0d\u8fc7\u90a3\u4e2a\u5e73\u5747\u5206\u5450\uff0c\u4ed6\u73b0\u5728\u521d\u4e8c\uff0c\u4f60\u53eb\u6211\u513f\u5b50\u600e\u4e48\u529e\u554a\uff1f\u4ed6\u73b0\u5728\u8fd8\u4e0d\u5230\u9ad8\u4e2d\u554a\uff1f\n\u4f60\u4eec\u5bb3\u6b7b\u6211\u513f\u5b50\u4e86\uff01\u5feb\u70b9\u51fa\u6765\u4f60\u8fd9\u4e2a\u7fa4\u4e3b\uff01\u518d\u8fd9\u6837\u6211\u53bb\u62a5\u8b66\u4e86\u554a\uff01\u6211\u8ddf\u4f60\u4eec\u8bf4\u4f60\u4eec\u8fd9\u4e00\u5e2e\u4eba\u554a\uff0c\u4e00\u5929\u5230\u665a\u554a\uff0c\n\u641e\u8fd9\u4e9b\u4ec0\u4e48\u6e38\u620f\u554a\uff0c\u52a8\u6f2b\u554a\uff0c\u4f1a\u5bb3\u6b7b\u4f60\u4eec\u7684\uff0c\u4f60\u4eec\u6ca1\u6709\u524d\u9014\u6211\u8ddf\u4f60\u8bf4\u3002\u4f60\u4eec\u8fd9\u4e5d\u767e\u591a\u4e2a\u4eba\uff0c\u597d\u597d\u5b66\u4e60\u4e0d\u597d\u5417\uff1f\n\u4e00\u5929\u5230\u665a\u5728\u4e0a\u7f51\u3002\u6709\u4ec0\u4e48\u610f\u601d\u554a\uff1f\u9ebb\u70e6\u4f60\u91cd\u89c6\u4e00\u4e0b\u4f60\u4eec\u7684\u751f\u6d3b\u7684\u76ee\u6807\u554a\uff1f\u6709\u4e00\u70b9\u5b66\u4e60\u76ee\u6807\u884c\u4e0d\u884c\uff1f\u4e00\u5929\u5230\u665a\u4e0a\u7f51\u662f\u4e0d\u662f\u4eba\u554a\uff1f\n</code></pre> \u8bf4\u8bdd\u4eba \u8f93\u5165\u97f3\u9891 \u5408\u6210\u97f3\u9891 \u7eb3\u897f\u59b2 (\u539f\u795e) \u9752\u96c0 (\u661f\u7a79\u94c1\u9053)"},{"location":"samples/#_2","title":"\u82f1\u6587\u53e5\u5b50","text":"<pre><code>In the realm of advanced technology, the evolution of artificial intelligence stands as a \nmonumental achievement. This dynamic field, constantly pushing the boundaries of what \nmachines can do, has seen rapid growth and innovation. From deciphering complex data \npatterns to driving cars autonomously, AI's applications are vast and diverse.\n</code></pre> \u8bf4\u8bdd\u4eba \u8f93\u5165\u97f3\u9891 \u5408\u6210\u97f3\u9891 Speaker 200 (LibriTTS) \u968f\u673a\u8bf4\u8bdd\u4eba 1  -  \u968f\u673a\u8bf4\u8bdd\u4eba 2  -"},{"location":"samples/#_3","title":"\u65e5\u6587\u53e5\u5b50","text":"<pre><code>\u5148\u9032\u6280\u8853\u306e\u9818\u57df\u306b\u304a\u3044\u3066\u3001\u4eba\u5de5\u77e5\u80fd\u306e\u9032\u5316\u306f\u753b\u671f\u7684\u306a\u6210\u679c\u3068\u3057\u3066\u7acb\u3063\u3066\u3044\u307e\u3059\u3002\u5e38\u306b\u6a5f\u68b0\u304c\u3067\u304d\u308b\u3053\u3068\u306e\u9650\u754c\u3092\n\u62bc\u3057\u5e83\u3052\u3066\u3044\u308b\u3053\u306e\u30c0\u30a4\u30ca\u30df\u30c3\u30af\u306a\u5206\u91ce\u306f\u3001\u6025\u901f\u306a\u6210\u9577\u3068\u9769\u65b0\u3092\u898b\u305b\u3066\u3044\u307e\u3059\u3002\u8907\u96d1\u306a\u30c7\u30fc\u30bf\u30d1\u30bf\u30fc\u30f3\u306e\u89e3\u8aad\u304b\n\u3089\u81ea\u52d5\u904b\u8ee2\u8eca\u306e\u64cd\u7e26\u307e\u3067\u3001AI\u306e\u5fdc\u7528\u306f\u5e83\u7bc4\u56f2\u306b\u53ca\u3073\u307e\u3059\u3002\n</code></pre> \u8bf4\u8bdd\u4eba \u8f93\u5165\u97f3\u9891 \u5408\u6210\u97f3\u9891 \u968f\u673a\u8bf4\u8bdd\u4eba 1  -  \u968f\u673a\u8bf4\u8bdd\u4eba 2  -"}]}